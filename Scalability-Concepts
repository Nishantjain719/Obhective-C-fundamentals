we could keep track of how busy each server(means keep track of how many requests each server is getting per unit time). and whoever has fewest at this instance wil receive the
next incoming request.
also think about how much work each connection involves on server ask questins like
1. what % of the cpu is actually being consumed right now
how much RAM is actually in use among those servers and pick lowest
so we have to anser about who is least busy or to whom sholud i send this request.
lest ask technical questions - 
how do you route request from client to server?
so why don't we just tell the client directly the ip address of whichever the server is currently the least busy? - we need to involve dns system in this(means DNS server answers the queries
of the form like what's the ip address of yourcompany.com based on currrent load) but here is fault with this design.
if you have isp's DNS server caching lookups as they do even its for some number of minutes the load on these servers is gonna be fluctuating quiety a bit
means if we ansering queries based on this moment of time and those answers are then reused for some number of minutes we might accidentaly end up overwhelming server 1 or 2, so
may be some users experience downtimes or delays even if server N is ideal and has resourses.
or in dns like if one server goes down and you still spiting out one ip address then 1/N of users might go dead end
so solution is load balancer - 
so it can perform routing function whereby requests just com to lb so lb represents the ip address of yourcompany.com
lb does not take into account of any kind of shared state that must be shared across diffenrt servers
lets talk about when we loging to web next time thereafter website remebers me that i logged in its not username or pass transmitted agin and again to remind server so cookie(fairly
large sudo random value) is passed and planted in my browser(in ram or disk) by server and server has another copy of that number may be in database(or in a file /Temp)
inside of that file or database is the stuff i want to rememberd about me(userbame, password, email) but storing cookies on disk seems to be bad because next server ay be doesn't 
know my session state. so just store cookies in mysql using key-valuse pairs beacuse /temp is unique per server.
but we are centraizing one db.
so we can use one other server anf has hom share his /temp directory to all the other servers like(NFS) DOWNSIDE - We are now centralizing our hardware with just one single file
server.
but its only serving files so may be it can handle more users but if that server dies we are screwed everything is down. because if 10 other server can't share sessions means no 
user is gonna able to login.
use caching engines(redis,etc) -
install cache server on listens on tcp port and you can plop it into memecached(relies on strings being values) generated html file by setting the value for given key what is key- 
if you have user profile then key will be userid(123) and value is just huge chunk of html. when code is to request user profile check cache if its in cache, grab and send it to 
browser else if its not in caache or its 10 minutes old so flush the cache and regenerate it by using normal db queries and save it agian in cache and spit out the results.

use mysql query cache - configure/edit a file on server(my_cnf). restart server. so in future when you exec sql queries the server wiil remember the rows that returend.
if you ask same query again it will return things from cache, and db itself keep track of which data is dirty(menas you have written to it), so cache should be invalidated.
so server will do all that for you.

in web you have mutile web servers and those have duplicate installations of your code.

we can use master-slave design - 
in which one or more slaves purpose in life is to read data that's been written to master, so they have copies of the master
so we could write code in such a way that any time you do insert, delete or modification to your db, you send those queries to master and if you do select or join do on any one 
of the slave dbs. so you can optimize one server for write and increase the capaciy you have for read.
but if we do select while a query is being replicated from master to slave- we use transction safe db(or LOCK) means either you read the whole row or non of it.
OTHER problem- if master goes down we can continue reading but no one can update suppose their fb profile untile we get that server backed up - solution
we can promote a slave to master using automatic scripts and human could interven and tweak conf.
moreover lb is sending Read and Write queries to MySql master and slaves(sql severs)  based on load or round-robin.
lb is single point failure - here is network level tricks menas we could have 2 lb or more side by side but they have only one ip adddress- 
means they send heartbeats to one and other which is like a ping from left to right and so on, so as soon as one of those servers does not hear heartbeat from the other
it assumes that he is down so remaining server take control of the ip at network level. so you have on ip but it floats across the servers and they decide dynamically
whether other guy is alive or not. or we have to coonect one lb to more swithes with ethenet cables ans so on, so awsor cloud is good because u get 
virtualization of these services.
for images u shuld do store paths or file names of files in db and store images file on disk or on web servers(CDN) for static contents.
you hosting it youself then cdn would live with web layer(means web servers) but typically they are outsourced  and u get just some unique long path.
.
partitioning - 
your MySql slaves for reading purposes are distributed and and dedicated to only subset of your users
nodejs - allows u yo write web servers in js and its executed server side. its event-driven so u wouldn't implement this idea of pooling by just sitting 
in loop(for, while) and sleeping.










